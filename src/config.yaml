shared:
  n_peaks_per_sample: 600  # 600 peaks ~= 10 minutes of recording
  # default set to comp_v_dpn (2 classes)
  class_config: "comp_v_dpn" # "all", "diab_v_comp", "comp_v_dpn"

model:
  n_peaks_per_sample: null # updated at runtime from shared config
  class_config: null # updated at runtime from shared config
  # default = sinusoidal for better generalization (and lower param count)
  pos_encoding_type: "sinusoidal"  # "learned" or "sinusoidal"
  dim_model: 128  # Transformer embedding size
  n_heads: 8  # Number of attention heads
  num_encoder_layers: 4  # Transformer encoder layers
  dim_feedforward: 512  # Feedforward dimension
  
  # Small constant added to denominators to prevent division by zero
  eps: 1.0e-8
  # Model hyperparameters
  dropout: 0.3    

  num_params: null # updated at runtime

data:
  # path to the hrv data directory
  hrv_data_dir: ""  # This needs to be specified at runtime
  n_peaks_per_sample: null # updated at runtime from shared config
  class_config: null # updated at runtime from shared config
  
  # split
  slice_strategy: "sliding"  # "sliding" or "chunked"
  data_config: nabian_w_index
  class_based_stride: [1, 1]

  # data loader config params
  train:
    batch_size: 8
    num_workers: 4
    shuffle: true
    pin_memory: false
    drop_last: true
  val:
    batch_size: 2
    num_workers: 4
    shuffle: false
    pin_memory: false
    drop_last: true

trainer:
  epochs: 100
  n_batches_train: null  # if not null, will only train on these many batches
  n_batches_val: null    # if not null, will only validate on these many batches
  device: "cuda"
  log_every_n_steps: 50
  use_validation: true
  run_validation_every_n_steps: 500  # is used if use_validation is true
  logs_dir: "logs"
  fold: 0
  
  # Optimization hyperparameters
  optim:
    lr: 2e-4  # Learning rate
    weight_decay: 1e-4  # Regularization
    use_lr_scheduler: false  # Whether to use learning rate scheduler

wandb:
  entity: "ad-aspera"
  project: "diabeat"
  name: null
  logs_dir: "logs"
