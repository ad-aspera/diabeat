model:
  # input length. should be equat to data.n_peaks_per_sample
  n_peaks_per_sample: 600  # Adjust based on HRV sequence length
  
  feature_dim: 128  # Transformer embedding size
  n_heads: 8  # Number of attention heads
  num_encoder_layers: 4  # Transformer encoder layers
  
  # Training Hyperparameters
  lr: 1e-3  # Learning rate
  weight_decay: 1e-5  # Regularization
  
  # Small constant added to denominators to prevent division by zero
  eps: 1.0e-8
  # Training hyperparameters
  dropout: 0.3    

  # populated automatically by trainer. set to null
  num_params: null

  optim:
    lr: 0.001

data:
  # path to the hrv data directory
  hrv_data_dir: null  # This needs to be specified at runtime
  # number of peaks per sample
  n_peaks_per_sample: 600  # 600 peaks ~= 10 minutes of recording

  # split
  split: "train"
  
  # data loader config params
  train:
    batch_size: 8
    num_workers: 4
    shuffle: true
    pin_memory: true

  val:
    batch_size: 8
    num_workers: 4
    shuffle: false
    pin_memory: true


trainer:
  accelerator: "gpu"
  devices: 1
  precision: 16
  strategy: "auto"
  epochs: 100
  log_every_n_steps: 50
  enable_progress_bar: true
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  overfit_batches: 0.0
  accumulate_grad_batches: 1
  detect_anomaly: false
  deterministic: false
  gradient_clip_val: null
  gradient_clip_algorithm: null
  checkpoint_id: null
  
  callbacks:
    early_stopping:
      enabled: false
      monitor: "val/loss"
      patience: 10
      min_delta: 0.0001
      mode: "min"
      verbose: true

    stochastic_weight_averaging:
      enabled: false
      lrs: 0.001
      swa_epoch_start: 0.8
      annealing_epochs: 5
      annealing_strategy: "cos"
      
    learning_rate_monitor:
      enabled: false
      logging_interval: "step"
      log_momentum: false

logger:
  # to log to proper wandb team
  team_name: "ad-aspera"
  # project name
  project_name: "diabeat"
  # if null, wandb will generate a random run name
  run_name: null
  # if not full path, wandb will assume root of project as base dir
  logs_dir: "logs"
  # checkpoint id is required if resuming from a checkpoint
  checkpoint_id: null


legacy_CNN_param:
  # Model architecture
  channels: [1, 32, 64, 128]  # [input, conv1, conv2, conv3]
  kernels: [15, 31, 61]  # [conv1, conv2, conv3]
  fc_dims: [64]  # Fully connected layer dimensions
  pool_size: 2
  stride: 1
  padding: "same"
  leaky_relu_slope: 0.01